# -*- coding: utf-8 -*-
"""label_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JyuVQbcdg8vx6dqr6O0prWrTt9d5-Uj

# Note
This script aims to divide the STAS into several classes based on their different features. In this script, we will use a pre-trained nn (**VGG16**) to extract a feature vector from images and cluster the images based on how similar the feature vectors are (using **KMeans** or **Hdbscan**).
"""

# Mount colab to our drive
from google.colab import drive
import os
drive.mount('/content/drive/')
os.chdir('/content/drive/My Drive/2022_趨勢_機器學習比賽/')

"""# Package introduction
⚫ **load_img** allows us to load an image from a file as a PIL object:

⚫ **img_to_array** allows us to convert the PIL object into a NumPy array
preproccess_input is meant to prepare your image into the format the model requires. You should load images with the Keras **load_img** function so that you guarantee the images you load are compatible with the **preprocess_input** function.

⚫ **VGG16** is the pre-trained model we’re going to use

⚫ **KMeans** the clustering algorithm we’re going to use

⚫ **hdbscan** another option for clustering

⚫ **PCA** for reducing the dimensions of our feature vector
"""

# for loading/processing the images  
from keras.preprocessing.image import load_img 
from keras.preprocessing.image import img_to_array 
from keras.applications.vgg16 import preprocess_input 

# models 
from keras.applications.vgg16 import VGG16 
from keras.models import Model
from keras.models import Sequential
from keras.layers.core import Flatten

# clustering and dimension reduction
!pip install hdbscan
import hdbscan
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# For standardization
from sklearn.preprocessing import StandardScaler

# For silhouette score method
from sklearn.metrics import silhouette_score

# for everything else
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from random import randint
import pandas as pd
import pickle

"""# Load data
we load the images that contains only STAS regions.
"""

img_file_path = 'OBJ_Train_Datasets/Train_images_hematoxylin_STAS_only/'
STAS = []

# creates a ScandirIterator aliased as files
with os.scandir(img_file_path) as files:
  # loops through each file in the directory
    for file in files:
        if file.name.endswith('.jpg'):
          # adds only the image files to the list
            STAS.append(file.name)
print(len(STAS))

"""# Data preprocessing and the model
**footnote_1**: Load the VGG model and remove the output layer manually. This means that the new final layer is a fully-connected layer with 4,096 output nodes. This vector of 4,096 numbers is the feature vector that we will use to cluster the images.

**footnote_2**: This is where we put the **load_img**() and **preprocess_input**() methods to use. When loading the images we are going to set the target size to (224, 224) because the VGG model expects the images it receives to be 224x224 NumPy arrays.

**footnote_3**: Now that the final layer is removed, we can pass our image through the predict method to get our feature vector.

"""

# load the model first and pass as an argument -footnote_1
model = VGG16(include_top=True)
model = Model(inputs = model.inputs, outputs = model.layers[-2].output)

def extract_features(file, model):
    # load the image as a 224x224 array -footnote_2
    img = load_img(file, target_size=(224,224))
    # convert from 'PIL.Image.Image' to numpy array -footnote_2
    img = np.array(img) 
    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels) -footnote_2
    reshaped_img = img.reshape(1,224,224,3) 
    # prepare image for model
    imgx = preprocess_input(reshaped_img)
    # get the feature vector  -footnote_3
    features = model.predict(imgx, use_multiprocessing=True)
    return features

"""# Extract the feature vectors from all of the images
Now we can use this feature_extraction function to extract the features from all of the images and store the features in a dictionary with filename as the keys.
"""

data = {}
count=0 # For checking the status of the loop

# lop through each image in the dataset
for STAS_obj in STAS:
    print(count)
    count+=1
    # try to extract the features and update the dictionary
    feat = extract_features(img_file_path+STAS_obj,model)
    data[STAS_obj] = feat

# get a list of the filenames
filenames = np.array(list(data.keys()))

# get a list of just the features.  
feat = np.array(list(data.values())) # (n_samples, 1, 4096 or 25088)

# reshape so that there are total samples of 4096 (or 25088) vectors. shape = (n_samples, 4096 or 25088)
feat = feat.reshape(-1,4096)

# Check the result
list_mode=[]
list_mode_count=[]
std_list=[]
std_m_list=[]
for kk in range(4096): # 4096 or 25088
  mode,mode_count = np.unique(feat[:,kk],return_counts=True);
  std = np.std(feat[:,kk])
  std_m = max(abs(feat[:,kk]-np.mean(feat[:,kk])))
  mode=mode[0]
  mode_count=mode_count[0]
  list_mode.append(mode)
  list_mode_count.append(mode_count)
  std_list.append(std)
  std_m_list.append(std_m)

print(set(list_mode))
# print(np.std(feat[:,221]))
# print(max(feat[:,221]))

# Testing ground

plt.figure(figsize=(10,8))
plt.hist(list_mode_count,100);
plt.xlabel('Num of samples shows 0 in that feature',size=30)
plt.ylabel('Num of features',size=30)
plt.title('2785 samples, 4096 features',size=30);

plt.figure(figsize=(20,8))
plt.subplot(121)
plt.scatter(list_mode_count,np.array(std_list)**2,marker='.')
plt.xlabel('Num of samples shows 0 in that feature',size=30)
plt.ylabel('variance',size=30)

plt.subplot(122)
plt.scatter(list_mode_count,np.array(std_m_list)**2/2785,marker='.')
plt.xlabel('Num of samples shows 0 in that feature',size=30)
plt.ylabel('square of the most distant outlier',size=30)

"""# Read my own features (README before executing!)
Read the features that were extracted manually. This step is independent of any previous process. This is another approach.
"""

import numpy as np
file_path = 'YC_feature_vectors.txt'
img_id = np.genfromtxt(file_path,skip_header=True,usecols=0,unpack=True,dtype=str)
img_id = [s +'.jpg' for s in img_id]
feature0 = np.genfromtxt(file_path,skip_header=True,usecols=1,unpack=True,dtype=float)
feature1 = np.genfromtxt(file_path,skip_header=True,usecols=2,unpack=True,dtype=float)
feature2 = np.genfromtxt(file_path,skip_header=True,usecols=3,unpack=True,dtype=float)
feature0 = feature0.reshape(-1,1)
feature1 = feature1.reshape(-1,1)
feature2 = feature2.reshape(-1,1)
feature_map = np.concatenate((feature0,feature1,feature2),axis=1)
print(feature_map.shape)

"""# Standerdization
We Standerdize the vectors here since PCA are distance-related and we are not sure which features are more important.

"""

Mm = StandardScaler()
feat_stand = Mm.fit_transform(feature_map)    # Note that the shape of the input should be: [n_samples, n_features]

# Check the result
print(np.mean(feat_stand[:,2]))

"""# Dimensionality Reduction (PCA)
Since our feature vector has over 4,000 dimensions, your computer will thank you if you reduce the number of dimensions from 4,000 to a much smaller number. We can't simply just shorten the list by slicing it or using some subset of it because we will lose information. If only there was a way to reduce the dimensionality while keeping as much information as possible.

The number of dimensions to reduce down to is up to you and I'm sure there's a method for finding the best number of components to use. I chose 100 as an arbitrary number in this case.
"""

# Dimensionality Reduction (PCA)

pca = PCA(n_components=0.6, random_state=0)
pca.fit(feat_stand)
x = pca.transform(feat_stand)

print(x.shape)

# Check the PCA output
list_a=[]
list_b=[]
for kk in range(x.shape[1]):
  a,b= np.unique(feat[:,kk],return_counts=True);
  a=a[0]
  b=b[0]
  list_a.append(a)
  list_b.append(b)

plt.figure(figsize=(10,8))
plt.hist(list_b,100);
plt.xlabel('Num of samples shows 0 in that feature',size=30)
plt.ylabel('Num of features',size=30)
plt.title(f'2785 samples, {x.shape[1]} features',size=30);

# print(a[0],b[0])
# print(np.std(feat[:,221]))
# print(max(feat[:,221]))

print(feat_stand.shape)

"""# KMeans
This algorithm will allow us to group our feature vectors into k clusters. Each cluster should contain images that are visually similar. Note that the choice of k is important, and we use **silhouette score method** to pick up the best k.
"""

# We standardize the input data before clustering. (Should we?)
Mm = StandardScaler()
x_stand = Mm.fit_transform(x)    # Note that the shape of the data: [n_samples, n_features]

# KMeans clustering
# Using silhouette score method to find the best k
k_list = [2,3,4,5,6,7,8,9,10]
silhouette_scores=[]
best_score=-1
for k in k_list:
  kmeans = KMeans(n_clusters=k , random_state=22) # set current hyper parameter
  kmeans.fit(feat_stand)                                   # fit model, (feat_stand or x)
  ss = silhouette_score(feat_stand, kmeans.labels_);     # calculate silhouette_score
  silhouette_scores+=[ss]                         # store all the scores
  print('Parameter:', k, 'Score', ss);
  if ss > best_score:
      best_score = ss
      best_k = k

n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters , random_state=22) # set current hyper parameter
kmeans.fit(feat_stand)                                   # fit model

# Show the KMeans label list. The order of the labels is parallel to the list of filenames for each image.
kmeans.labels_
color = cm.rainbow(np.linspace(0,1,n_clusters))
for i, c in zip(range(n_clusters),color):
  plt.scatter(feat_stand[kmeans.labels_==i,0],feat_stand[kmeans.labels_==i,1],color=c,marker='.');
# print(set(x[kmeans.labels_[kmeans.labels_==i]][:,6]))
# print(len(set(x[kmeans.labels_==i,0])))

"""# Hdbscan
Another algorithm for clustering \
ref: https://axk51013.medium.com/2021年資料科學家必備分群法hdbscan簡介-fba8287e666c
"""

# We standardize the input data before clustering. (Should we?)
Mm = StandardScaler()
x_stand = Mm.fit_transform(x)    # Note that the shape of the data: [n_samples, n_features]

# HDBSCAN clustering
hclusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(x)

# Show the DBSCAN label list. The order of the labels is parallel to the list of filenames for each image.
a=set(hclusterer.labels_)
print(a)
print(len(hclusterer.labels_[hclusterer.labels_==-1]))

# Show the HDBSCAN label list. The order of the labels is parallel to the list of filenames for each image.
color = cm.rainbow(np.linspace(0,1,2))
for i, c in zip(range(2),color):
  plt.scatter(x[hclusterer.labels_==i,0],x[hclusterer.labels_==i,1],color=c,marker='.');
# print(set(x[kmeans.labels_[kmeans.labels_==i]][:,6]))
# print(len(set(x[kmeans.labels_==i,0])))

"""# Group the clustering result
Now we can group the images into their clusters.
"""

# holds the cluster id and the images { id: [images] }
groups = {}
for file, cluster in zip(img_id,kmeans.labels_): # filenames or img_id. kmeans.labels_ or hclusterer.labels_ , depending on what clustering method you use.
    if cluster not in groups.keys():
        groups[cluster] = []
        groups[cluster].append(file)
    else:
        groups[cluster].append(file)

"""# Clone data
Now we can clone the images to the folders based on the clustering result.
"""

import shutil

src = 'OBJ_Train_Datasets/Train_images/'
cluster_num=range(n_clusters)
for num in cluster_num:
  dst = f'OBJ_Train_Datasets/Train_images_STAS_cluster/c{num}/'
  for img_num in groups[num]:
    # shutil.copy(src+img_num, dst+img_num)
    continue 
  print(groups[num])

import shutil
src = 'OBJ_Train_Datasets/garbage_truck/'
dst = 'OBJ_Train_Datasets/dogs/'
for img_num in groups[1]:
  # shutil.move(src+img_num,dst+img_num)
  continue

"""# Save the clustering result
We output the clustering result as a txt file for the following YOLOv5 training.
"""

target_path = 'OBJ_Train_Datasets/Train_Images/cluster_result.txt'
cluster_result=[]
for gp in range(len(groups)):
  for j in groups[gp]:
    gp_num = gp+1
    img_name = j.split('_')[0]
    img_STAS_num = j.split('_')[1].split('.')[0]
    cluster_result.append(f'{img_name} {img_STAS_num} {gp_num} \n')
f = open(target_path,'w+')
f.writelines(cluster_result)
f.close()

groups[0][1].split('_')[1].split('.')

"""# Check the clustering result
We plot the images in each cluster to check the result.
"""

import random
# function that lets you view a cluster (based on identifier)        
def view_cluster(cluster):
    plt.figure(figsize = (25,25));
    # gets the list of filenames for a cluster
    files = groups[cluster]
    random.shuffle(files)
    # only allow up to 30 images to be shown at a time
    if len(files) > 30:
        print(f"This is cluster {cluster}. Clipping cluster size from {len(files)} to 30")
        files = files[:30]
    # plot each image in the cluster
    for index, file in enumerate(files):
        plt.subplot(10,10,index+1);
        img = load_img(img_file_path+file)
        img = np.array(img)
        plt.imshow(img)
        plt.axis('off')

view_cluster(0)

view_cluster(1)

view_cluster(2)

view_cluster(3)

view_cluster(4)

view_cluster(5)

view_cluster(6)

view_cluster(7)

view_cluster(8)

view_cluster(9)