{"cells":[{"cell_type":"markdown","metadata":{"id":"PKKHBHBj0yXA"},"source":["# Note\n","This script process our training data to fit the required structure for using YOLOv5. First, we mount the machine to our drive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25113,"status":"ok","timestamp":1653550291820,"user":{"displayName":"YC Su","userId":"15182760599542063976"},"user_tz":-480},"id":"vp6oP4AW1X4a","outputId":"7dfdc97a-8413-4215-f9a3-3c5c5cabd045"},"outputs":[],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/2022_趨勢_機器學習比賽/')"]},{"cell_type":"markdown","metadata":{"id":"ilhoqmv910eU"},"source":["# Create validation set\n","We divide our dataset into training set and validation set. The list of the number of the validation set is at\n","OBJ_Train_Datasets/STAS_YOLOv5/images/val_set.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1653550625319,"user":{"displayName":"YC Su","userId":"15182760599542063976"},"user_tz":-480},"id":"yb4-lh771ueD","outputId":"b5412c07-9956-45f0-a585-710426030e2f"},"outputs":[],"source":["import random\n","num_list=[f'{i:08}' for i in range(1053)] # We have 1052 training images.\n","random.Random(0).shuffle(num_list)\n","val_list=num_list[:105] #1/10 of training set\n","tr_list=num_list[105:]\n","print(val_list)"]},{"cell_type":"markdown","metadata":{"id":"BX8nXYfY4Qly"},"source":["# **Copy the images**\n","We copy the training images to our target directories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u844Yv-U1ute"},"outputs":[],"source":["import shutil\n","orginal_location='./OBJ_Train_Datasets/Train_Images/'\n","tr_img_location='./OBJ_Train_Datasets/STAS_YOLOv5/images/train/'\n","val_img_location='./OBJ_Train_Datasets/STAS_YOLOv5/images/val/'\n","\n","for num in tr_list:\n","  img=orginal_location+f'{num}.jpg'\n","  shutil.copyfile(img, tr_img_location+f'{num}.jpg')\n","for num in val_list:\n","  img=orginal_location+f'{num}.jpg'\n","  shutil.copyfile(img, val_img_location+f'{num}.jpg')"]},{"cell_type":"markdown","metadata":{"id":"nPzKqDgA869I"},"source":["# Create the txt file\n","We create the required text file for YOLOv5."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":744376,"status":"ok","timestamp":1653551485849,"user":{"displayName":"YC Su","userId":"15182760599542063976"},"user_tz":-480},"id":"bBZ00uzlANkI"},"outputs":[],"source":["import numpy as np\n","import xml.etree.ElementTree as ET\n","\n","def create_txt(mode,label=False): # 'tr' or 'val'\n","  if mode == 'tr':\n","    output_location = './OBJ_Train_Datasets/STAS_YOLOv5/labels/train/'\n","    l = tr_list\n","  elif mode == 'val':\n","    output_location = './OBJ_Train_Datasets/STAS_YOLOv5/labels/val/'\n","    l = val_list\n","  else:\n","    print('the input argument is wrong, please check!')\n","    return None\n","\n","  # Read the YC labels\n","  if label:\n","    YC_label_path = './OBJ_Train_Datasets/Train_Images/cluster_result.txt'\n","    YC_img_list = np.genfromtxt(YC_label_path,usecols=0,unpack=True,skip_header=False,dtype=str)\n","    YC_count_list = np.genfromtxt(YC_label_path,usecols=1,unpack=True,skip_header=False,dtype=int)\n","    YC_label_list = np.genfromtxt(YC_label_path,usecols=2,unpack=True,skip_header=False,dtype=int)\n","\n","  x_norm, y_norm = 1716, 942\n","\n","  for num in l:\n","    ann_file=f'OBJ_Train_Datasets/Train_Annotations/{num}.xml'\n","    output=f'{output_location}{num}.txt'\n","    label_list=[]\n","    \n","    # Read the position of the STAS from the annotations file\n","    tree = ET.parse(ann_file)\n","    root = tree.getroot()\n","    STAS_position_dict={} # Iinitialize the dictionary\n","    count = 0 # Initialize the counting (meaningfule under the YC lebels case)\n","    for obj in root.findall('object'):\n","      xmin = int(obj.find('bndbox').find('xmin').text)\n","      ymin = int(obj.find('bndbox').find('ymin').text)\n","      xmax = int(obj.find('bndbox').find('xmax').text)\n","      ymax = int(obj.find('bndbox').find('ymax').text)\n","      \n","      width = np.round((xmax-xmin)/x_norm,6)\n","      height = np.round((ymax-ymin)/y_norm,6)\n","      x_cen = np.round((xmax+xmin)/(2*x_norm),6)\n","      y_cen = np.round((ymax+ymin)/(2*y_norm),6)\n","\n","      if label:\n","        YC_label_index = np.where(YC_count_list[YC_img_list==num]==count)[0][0]\n","        YC_label = YC_label_list[YC_img_list==num][YC_label_index] - 1\n","        count+=1\n","        label_list.append(f'{YC_label} {x_cen} {y_cen} {width} {height} \\n')\n","      else:\n","        label_list.append(f'0 {x_cen} {y_cen} {width} {height} \\n')\n","    \n","    f = open(output,'w+')\n","    f.writelines(label_list)\n","    f.close()\n","\n","  return None\n","create_txt('tr',label=False)\n","create_txt('val',label=False)"]},{"cell_type":"markdown","metadata":{"id":"xFBcSJX2yr8F"},"source":["# **Create feature vectors manually**\n","In this block we design the feature vectors manually for the following clustering method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yx1ITrCF8xQb"},"outputs":[],"source":["# Read the dataset and return the image as a 3D array and a dictionary contains \n","# all important information listed in the annotation files.\n","# Be aware that \n","# (1) the color order of the img array is BGR if you use cv2.imread.\n","# (2) the dimensions of the image are ordered in (y,x,color).\n","import cv2\n","import xml.etree.ElementTree as ET\n","\n","def Read(img_num,read_img=True):\n","  img_file=f'OBJ_Train_Datasets/Train_Images/{img_num}.jpg'\n","  ann_file=f'OBJ_Train_Datasets/Train_Annotations/{img_num}.xml'\n","  tree = ET.parse(ann_file)\n","  root = tree.getroot()\n","  inf_dict={}\n","  count=0\n","\n","  for obj in root.findall('object'):\n","    xmin=int(obj.find('bndbox').find('xmin').text)\n","    ymin=int(obj.find('bndbox').find('ymin').text)\n","    xmax=int(obj.find('bndbox').find('xmax').text)\n","    ymax=int(obj.find('bndbox').find('ymax').text)\n","    inf_dict[count]=[(xmin,ymin),(xmax,ymax)]\n","    count+=1\n","\n","  inf_dict['STAS_num']=count\n","  inf_dict['xsize']=root.find('size').find('width').text\n","  inf_dict['ysize']=root.find('size').find('height').text\n","  inf_dict['zsize']=root.find('size').find('depth').text\n","\n","  if read_img:\n","    img=cv2.imread(img_file)\n","    return img, inf_dict\n","  else:\n","    return inf_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLdsQSrlysMA"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","\n","output_path = 'YC_feature_vectors.txt'\n","\n","# Size, shape, neighborhood\n","img_num_list=[f'{i:08}' for i in range(1053)] # We have 1053 training images\n","# random.shuffle(img_num_list)\n","feature_list=[]\n","img_size=1716*942\n","extend = 10   # Use to recognize the surroudings of STASs.\n","# Number of STAS in each image, size and shape distibution\n","for img_num in img_num_list:\n","  print(f'Dealing with image {img_num} now....')\n","  img, inf = Read(img_num,read_img=True)\n","  for obj_num in range(inf['STAS_num']):\n","    # neighborhood. We count the pixel ratio of the white in surroundings.\n","    x_ind1,x_ind2=inf[obj_num][0][0],inf[obj_num][1][0]\n","    y_ind1,y_ind2=inf[obj_num][0][1],inf[obj_num][1][1]\n","    xmin=max(x_ind1-extend,0)\n","    ymin=max(y_ind1-extend,0)\n","    xmax=min(x_ind2+extend,1716)\n","    ymax=min(y_ind2+extend,942)\n","    img_sur = img[ymin:ymax,xmin:xmax,:].astype(float)\n","    img_sur[extend:-extend,extend:-extend,:] = np.nan\n","    index = np.nansum(img_sur,axis=2) < 650\n","    img_masked = img[ymin:ymax,xmin:xmax,:].astype('float')\n","    img_masked[index,:] = np.nan\n","    total_sur_area=img_sur[:,:,0].size-np.sum(np.isnan(img_sur[:,:,0]))\n","    white_sur_area=img_masked[:,:,0].size-np.sum(np.isnan(img_masked[:,:,0]))\n","    area_ratio = white_sur_area/total_sur_area\n","\n","    xlen,ylen=x_ind2-x_ind1,y_ind2-y_ind1\n","    area=xlen*ylen/img_size\n","    shape=xlen/ylen\n","\n","    feature_list.append(f'{img_num}_{obj_num} {area_ratio} {area} {shape} \\n')\n","\n","f = open(output_path,'w')\n","f.writelines(feature_list)\n","f.close();"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Preprocessing_for_YOLOv5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
